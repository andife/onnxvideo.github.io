<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>https://andife.github.io/</title><link>https://andife.github.io/onnxvideo.github.io/</link><description></description><atom:link href="https://andife.github.io/onnxvideo.github.io/feeds/speaker_lukasz-langa.rss.xml" rel="self"></atom:link><lastBuildDate>Wed, 20 Nov 2019 00:00:00 +0000</lastBuildDate><item><title>AsyncIO and Music</title><link>https://andife.github.io/onnxvideo.github.io/codedive-2019/asyncio-and-music.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Can Python help a musician play hardware instruments? Is there anything specific about AsyncIO that makes it well suited to the task? Come see how AsyncIO can be used to aid music production and realtime performance through MIDI processing.&lt;/p&gt;
&lt;p&gt;First, we will cover the transformations of the incoming MIDI signal, that is helping the musician play his hardware instruments in new, unique ways. Examples of such transformations we will cover are arpeggiators, MIDI channel multiplexers, legato-based portamento.&lt;/p&gt;
&lt;p&gt;Then we'll move onto generative music, that is sequences generated procedurally. The musician specifies the tempo and the music scale, and a Python program generates music progressions on its own.&lt;/p&gt;
&lt;p&gt;We'll close with some thoughts on audio signal processing and the challenges faced there.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">≈Åukasz Langa</dc:creator><pubDate>Wed, 20 Nov 2019 00:00:00 +0000</pubDate><guid>tag:andife.github.io,2019-11-20:onnxvideo.github.io/codedive-2019/asyncio-and-music.html</guid></item></channel></rss>